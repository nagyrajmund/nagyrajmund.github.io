<!DOCTYPE html>
<html>

<head>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="stylesheet" href="style.css">
	<title>Rajmund Nagy</title>
</head>

<body>
	<div class="prof_pic_container">
		<img src="assets/prof_pic.jpeg" alt="Profile Picture">
	</div>
	<!-- <img id="prof_pic_conmt" src="assets/prof_pic.jpeg" alt="Rajmund Nagy"> -->

	<h1>Rajmund Nagy</h1>

	<h3>
		<a href="https://scholar.google.com/citations?user=Lm-asRsAAAAJ">scholar</a>
		| <a href="https://github.com/nagyrajmund">github</a>
		| <a href="https://www.linkedin.com/in/rajmundn/">linkedin</a>
		| <a href="mailto:rajmundn@kth.se">email</a>
	</h3>

	<p>
		I am a third-year PhD student at KTH Royal Institute of Technology, advised by <a
			href="https://people.kth.se/~ghe/">Gustav Eje Henter</a>. My background is in mathematical modelling,
		software engineering, and machine learning.
	</p>

	<!--<p>-->
	<!--I am interested in deep learning research, with a recent focus on foundation models. In my past work, I have enjoyed forays into noisy data learning, graph ML, neuro-symbolic AI, and program analysis.-->
	<!--</p>-->
	<h2>Research</h2>

	My research focuses on <b>3D character animation</b> and <b>deep generative models</b>, with a central question:
	<i>how can we create
		immersive 3D character experiences that resonate with audiences at scale?</i> To explore this, I record real
	human performances using
	<b>motion
		capture</b>, and develop generative models for <b>automated,
		controllable
		motion synthesis</b>. By conducting <b>large-scale human evaluations</b>, I aim to identify the strengths and
	limitations of current systems, with the goal of improving them for real-world applications.

	<h2>Diary</h2>

	<table>

		<tr>
			<th>Nov 4, 2024</th>
			<td>I will co-organise the fifth GENEA workshop on the <a
					href="https://genea-workshop.github.io/2024/workshop/">Generation and Evaluation of Non-verbal
					Behaviour for Embodied Agents</a> at ICMI 2024 in San José, Costa Rica.</td>
		</tr>

		<tr>
			<th><span style="white-space: nowrap;">Sep 29, 2024</span></th>
			<td>Co-organised the first workshop on <a
					href="https://expressive-encounters-workshop.github.io/2024/">Expressive Encounters: Co-speech
					gestures across cultures in the wild</a> at ECCV 2024 in Milan, Italy.</td>
		</tr>

		<tr>
			<th><span style="white-space: nowrap;">Aug 19, 2024</span></th>
			<td>Co-organised a study trip to various labs in Japan – RIKEN, NII, Google DeepMind, CyberAgent, Kyoto
				University, and Nagoya University – for a group of Swedish PhD students working on machine learning for
				media applications. (<a
					href="https://www.linkedin.com/posts/wara-media-and-language_ai-wasp-waraml-activity-7239528164467634177-AymE">LinkedIn
					post</a>)</td>
		</tr>


		<tr>
			<th>Oct 9, 2023</th>
			<td>Co-organised the fourth GENEA workshop on the <a
					href="https://genea-workshop.github.io/2024/workshop/">Generation and Evaluation of Non-verbal
					Behaviour for Embodied Agents</a> at ICMI 2023 in Paris, France.</td>
		</tr>

		<tr>
			<th>Aug 7, 2023</th>
			<td>I presented our work <a href="https://www.speech.kth.se/research/listen-denoise-action/">Listen,
					denoise, action! Audio-driven motion synthesis with diffusion models</a> at SIGGRAPH 2023 in Los
				Angeles, USA.</td>
		</tr>

		<tr>
			<th>May 2, 2023</th>
			<td>We launched <a href="https://genea-workshop.github.io/2023/challenge/">the third GENEA Challenge</a>,
				focused on generating and evaluating synthetic gestures in dyadic conversations, with 12 submitting
				teams in total!</td>
		</tr>

		<tr>
			<th>Oct 11, 2022</th>
			<td>Started my PhD at KTH.</td>
		</tr>

	</table>


	<!--<h2>pre-prints</h2>-->
	<!---->
	<!--(* denotes equal contribution)-->

	<!--<ol>-->


	<!--</ol>-->


	<h2>Publications</h2>

	(* denotes equal contribution)

	<ol>
		<li>
			<b>The GENEA Challenge 2023: A large-scale evaluation of gesture generation models in monadic and dyadic
				settings</b><br>
			Taras Kucherenko*, <u>Rajmund Nagy</u>*, Youngwoo Yoon*, Jieyeon Woo, Teodor Nikolov, Mihail Tsakov, Gustav
			Eje
			Henter<br>
			<i>ICMI 2023</i><br>

			<a href="https://arxiv.org/abs/2308.12646">paper</a>
			| <a href="https://svito-zar.github.io/GENEAchallenge2023/">website</a>
			| <a href="assets/2023_genea/cite.txt">citation</a>
		</li>

		<li>
			<b>Listen, denoise, action! Audio-driven motion synthesis with diffusion models</b><br>
			Simon Alexanderson, <u>Rajmund Nagy</u>, Jonas Beskow, Gustav Eje Henter<br>
			<i>SIGGRAPH 2023 and ACM Transactions on Graphics (TOG)</i><br>

			<a href="https://arxiv.org/abs/2211.09707">paper</a>
			| <a href="https://www.speech.kth.se/research/listen-denoise-action/">website</a>
			| <a href="https://github.com/simonalexanderson/ListenDenoiseAction">code</a>
			| <a href="assets/2023_lda/cite.txt">citation</a>
		</li>

		<li>
			<b>Multimodal analysis of the predictability of hand-gesture properties</b><br>
			Taras Kucherenko, <u>Rajmund Nagy</u>, Michael Neff, Hedvig Kjellström, Gustav Eje Henter<br>
			<i>AAMAS 2022</i><br>

			<a href="https://arxiv.org/abs/2108.05762">paper</a>
			| <a href="assets/2022_properties/cite.txt">citation</a>
		</li>

		<li>
			<b>Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for Generating Representational
				Gestures from Speech</b><br>
			<i>IVA 2021 - extended abstract</i><br>

			<a href="https://arxiv.org/abs/2106.14736">paper</a>
			| <a href="https://svito-zar.github.io/speech2properties2gestures/">website</a>
			| <a href="assets/2021_s2p2g/cite.txt">citation</a>
		</li>


		<li>
			<b>A Framework for Integrating Gesture Generation Models into Interactive Conversational Agents</b><br>
			<u>Rajmund Nagy</u>*, Taras Kucherenko*, Birger Moell, André Pereira, Hedvig Kjellström, Ulysses
			Bernardet<br>
			<i>AAMAS 2021 - demo track</i><br>

			<a href="https://arxiv.org/abs/2102.12302">paper</a>
			| <a href="assets/2023_lda/cite.txt">citation</a>
		</li>

	</ol>

</body>

</html>